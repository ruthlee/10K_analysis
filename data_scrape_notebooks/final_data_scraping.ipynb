{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm # for i in tqdm(list): gives progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10K Similarity Analysis\n",
    "Goals: we want to scrape 10K, 10Q data from SEC Edgar with certain ticker symbols, then do a similarity analysis of those documents year over year to analyze share price correlations per \"Lazy Prices.\" Slightly edited and taken from: https://www.quantopian.com/posts/scraping-10-ks-and-10-qs-for-alpha \n",
    "$$\n",
    "$$\n",
    "We don't want to do as much as that notebook did, however, because we don't have that kind of space on this machine. WIll edit as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "Input tickers of interest and automatically save the plain text files of the relevant 10Ks and 10Qs. Since the SEC has their own internal company identifier (CIK = \"Central Index Key\"), we have to first map the tickers we want to the relevant CIK.\n",
    "\n",
    "The original source for the ticker to CIK code is here: https://gist.github.com/dougvk/8499335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TickertoCIK(tickers):\n",
    "    url = 'http://www.sec.gov/cgi-bin/browse-edgar?CIK={}&Find=Search&owner=exclude&action=getcompany'\n",
    "    cik_re = re.compile(r'.*CIK=(\\d{10}).*')\n",
    "\n",
    "    cik_dict = {}\n",
    "    for ticker in tqdm(tickers): # Use tqdm lib for progress bar\n",
    "        results = cik_re.findall(requests.get(url.format(ticker)).text)\n",
    "        if len(results):\n",
    "            cik_dict[str(ticker).lower()] = str(results[0])\n",
    "    \n",
    "    return cik_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "cik_dict = TickertoCIK(['nvda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nvda</td>\n",
       "      <td>0001045810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker         cik\n",
       "0   nvda  0001045810"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now make it into a dataframe so it's compatible with the below function\n",
    "ticker_cik_df = pd.DataFrame.from_dict(data = cik_dict, orient='index')\n",
    "ticker_cik_df.reset_index(inplace=True)\n",
    "ticker_cik_df.columns = ['ticker', 'cik']\n",
    "ticker_cik_df['cik'] = [str(cik) for cik in ticker_cik_df['cik']]\n",
    "ticker_cik_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WriteLogFile(log_file_name, text):\n",
    "    \n",
    "    '''\n",
    "    Helper function.\n",
    "    Writes a log file with all notes and\n",
    "    error messages from a scraping \"session\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "    text : str\n",
    "        Text to write to the log file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    with open(log_file_name, \"a\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname_10k = '/Users/Kyelee/MyProjects/10K_NLP/10K'\n",
    "pathname_10q = '/Users/Kyelee/MyProjects/10K_NLP/10Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Scrape10K(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Ks and 10-K405s for a particular \n",
    "    CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(cik)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    os.chdir(cik)\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    # Request list of 10-K filings\n",
    "    res = requests.get(browse_url_base % cik)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.chdir('..')\n",
    "        os.rmdir(cik) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "\n",
    "    # If the request doesn't fail, continue...\n",
    "    \n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables)<3:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-K and 10-K405 document filings\n",
    "    filings_table = filings_table[(filings_table['Filings'] == '10-K') | (filings_table['Filings'] == '10-K405')]\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing\n",
    "        if docs_page.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "\n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            os.chdir('..')\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname))\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-K directory\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Scrape10Q(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Qs for a particular CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(cik)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    os.chdir(cik)\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    # Request list of 10-Q filings\n",
    "    res = requests.get(browse_url_base % cik)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.chdir('..')\n",
    "        os.rmdir(cik) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "    \n",
    "    # If the request doesn't fail, continue...\n",
    "\n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables)<3:\n",
    "        print(\"table too short\")\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-Q document filings\n",
    "    filings_table = filings_table[filings_table['Filings'] == '10-Q']\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing    \n",
    "        if docs_page.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "            \n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[docs_table['Type'] == '10-Q']\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            os.chdir('..')\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname))\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "            \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-Q directory\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001045810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Ks\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-K'\n",
    "filing_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Set correct directory\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "with open(log_file_name, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    Scrape10K(browse_url_base=browse_url_base_10k, \n",
    "          filing_url_base=filing_url_base_10k, \n",
    "          doc_url_base=doc_url_base_10k, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001045810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Qs\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10q = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-Q&count=1000'\n",
    "filing_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Set correct directory (fill this out yourself!)\n",
    "os.chdir(pathname_10q)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "log_file = open(log_file_name, 'a')\n",
    "log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Qs\n",
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    Scrape10Q(browse_url_base=browse_url_base_10q, \n",
    "          filing_url_base=filing_url_base_10q, \n",
    "          doc_url_base=doc_url_base_10q, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoveNumericalTables(soup):\n",
    "    \n",
    "    '''\n",
    "    Removes tables with >15% numerical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup\n",
    "        with numerical tables removed.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Determines percentage of numerical characters\n",
    "    # in a table\n",
    "    def GetDigitPercentage(tablestring):\n",
    "        if len(tablestring)>0.0:\n",
    "            numbers = sum([char.isdigit() for char in tablestring])\n",
    "            length = len(tablestring)\n",
    "            return numbers/length\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # Evaluates numerical character % for each table\n",
    "    # and removes the table if the percentage is > 15%\n",
    "    [x.extract() for x in soup.find_all('table') if GetDigitPercentage(x.get_text())>0.15]\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoveTags(soup):\n",
    "    \n",
    "    '''\n",
    "    Drops HTML tags, newlines and unicode text from\n",
    "    filing text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------a\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Filing text.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Remove HTML tags with get_text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Replace unicode characters with their\n",
    "    # \"normal\" representations\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvertHTML(cik):\n",
    "    \n",
    "    '''\n",
    "    Removes numerical tables, HTML tags,\n",
    "    newlines, unicode text, and XBRL tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik : str\n",
    "        Central Index Key used to scrape files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Look for files scraped for that CIK\n",
    "    try: \n",
    "        os.chdir(cik)\n",
    "    # ...if we didn't scrape any files for that CIK, exit\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find directory for CIK\", cik)\n",
    "        return\n",
    "        \n",
    "    print(\"Parsing CIK %s...\" % cik)\n",
    "    parsed = False # flag to tell if we've parsed anything\n",
    "    \n",
    "    # Try to make a new directory within the CIK directory\n",
    "    # to store the text representations of the filings\n",
    "    try:\n",
    "        os.mkdir('rawtext')\n",
    "    # If it already exists, continue\n",
    "    # We can't exit at this point because we might be\n",
    "    # partially through parsing text files, so we need to continue\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # Get list of scraped files\n",
    "    # excluding hidden files and directories\n",
    "    file_list = [fname for fname in os.listdir() if not (fname.startswith('.') | os.path.isdir(fname))]\n",
    "    \n",
    "    # Iterate over scraped files and clean\n",
    "    for filename in file_list:\n",
    "            \n",
    "        # Check if file has already been cleaned\n",
    "        new_filename = filename.replace('.html', '.txt')\n",
    "        text_file_list = os.listdir('rawtext')\n",
    "        if new_filename in text_file_list:\n",
    "            continue\n",
    "        \n",
    "        # If it hasn't been cleaned already, keep going...\n",
    "        \n",
    "        # Clean file\n",
    "        with open(filename, 'r') as file:\n",
    "            parsed = True\n",
    "            soup = bs.BeautifulSoup(file.read(), \"lxml\")\n",
    "            soup = RemoveNumericalTables(soup)\n",
    "            text = RemoveTags(soup)\n",
    "            with open('rawtext/'+new_filename, 'w') as newfile:\n",
    "                newfile.write(text)\n",
    "    \n",
    "    # If all files in the CIK directory have been parsed\n",
    "    # then log that\n",
    "    if parsed==False:\n",
    "        print(\"Already parsed CIK\", cik)\n",
    "    \n",
    "    os.chdir('..')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing CIK 0001045810...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# For 10-Ks...\n",
    "\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    ConvertHTML(cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing CIK 0001045810...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# For 10-Qs...\n",
    "\n",
    "os.chdir(pathname_10q)\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    ConvertHTML(cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeCosineSimilarity(words_A, words_B):\n",
    "    \n",
    "    '''\n",
    "    Compute cosine similarity between document A and\n",
    "    document B.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words_A : set\n",
    "        Words in document A.\n",
    "    words_B : set\n",
    "        Words in document B\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cosine_score : float\n",
    "        Cosine similarity between document\n",
    "        A and document B.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Compile complete set of words in A or B\n",
    "    words = list(words_A.union(words_B))\n",
    "    \n",
    "    # Determine which words are in A\n",
    "    vector_A = [1 if x in words_A else 0 for x in words]\n",
    "    \n",
    "    # Determine which words are in B\n",
    "    vector_B = [1 if x in words_B else 0 for x in words]\n",
    "    \n",
    "    # Compute cosine score using scikit-learn\n",
    "    array_A = np.array(vector_A).reshape(1, -1)\n",
    "    array_B = np.array(vector_B).reshape(1, -1)\n",
    "    cosine_score = cosine_similarity(array_A, array_B)[0,0]\n",
    "    \n",
    "    return cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeJaccardSimilarity(words_A, words_B):\n",
    "    \n",
    "    '''\n",
    "    Compute Jaccard similarity between document A and\n",
    "    document B.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words_A : set\n",
    "        Words in document A.\n",
    "    words_B : set\n",
    "        Words in document B\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    jaccard_score : float\n",
    "        Jaccard similarity between document\n",
    "        A and document B.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Count number of words in both A and B\n",
    "    words_intersect = len(words_A.intersection(words_B))\n",
    "    \n",
    "    # Count number of words in A or B\n",
    "    words_union = len(words_A.union(words_B))\n",
    "    \n",
    "    # Compute Jaccard similarity score\n",
    "    jaccard_score = words_intersect / words_union\n",
    "    \n",
    "    return jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeSimilarityScores10K(cik):\n",
    "    \n",
    "    '''\n",
    "    Computes cosine and Jaccard similarity scores\n",
    "    over 10-Ks for a particular CIK.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik: str\n",
    "        Central Index Key used to scrape and name\n",
    "        files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Open the directory that holds plaintext\n",
    "    # filings for the CIK\n",
    "    os.chdir(cik+'/rawtext')\n",
    "    print(\"Parsing CIK %s...\" % cik)\n",
    "    \n",
    "    # Get list of files to over which to compute scores\n",
    "    # excluding hidden files and directories\n",
    "    file_list = [fname for fname in os.listdir() if not \n",
    "                 (fname.startswith('.') | os.path.isdir(fname))]\n",
    "    file_list.sort()\n",
    "    \n",
    "    # Check if scores have already been calculated...\n",
    "    try:\n",
    "        os.mkdir('../metrics')\n",
    "    # ... if they have been, exit\n",
    "    except OSError:\n",
    "        print(\"Already parsed CIK %s...\" % cik)\n",
    "        os.chdir('../..')\n",
    "        return\n",
    "    \n",
    "    # Check if enough files exist to compute sim scores...\n",
    "    # If not, exit\n",
    "    if len(file_list) < 2:\n",
    "        print(\"No files to compare for CIK\", cik)\n",
    "        os.chdir('../..')\n",
    "        return\n",
    "    \n",
    "    # Initialize dataframe to store sim scores\n",
    "    dates = [x[-14:-4] for x in file_list]\n",
    "    cosine_score = [0]*len(dates)\n",
    "    jaccard_score = [0]*len(dates)\n",
    "    data = pd.DataFrame(columns={'cosine_score': cosine_score, \n",
    "                                 'jaccard_score': jaccard_score},\n",
    "                       index=dates)\n",
    "        \n",
    "    # Open first file\n",
    "    file_name_A = file_list[0]\n",
    "    with open(file_name_A, 'r') as file:\n",
    "        file_text_A = file.read()\n",
    "        \n",
    "    # Iterate over each 10-K file...\n",
    "    for i in range(1, len(file_list)):\n",
    "\n",
    "        file_name_B = file_list[i]\n",
    "\n",
    "        # Get file text B\n",
    "        with open(file_name_B, 'r') as file:\n",
    "            file_text_B = file.read()\n",
    "\n",
    "        # Get set of words in A, B\n",
    "        words_A = set(re.findall(r\"[\\w']+\", file_text_A))\n",
    "        words_B = set(re.findall(r\"[\\w']+\", file_text_B))\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        cosine_score = ComputeCosineSimilarity(words_A, words_B)\n",
    "        jaccard_score = ComputeJaccardSimilarity(words_A, words_B)\n",
    "\n",
    "        # Store score values\n",
    "        date_B = file_name_B[-14:-4]\n",
    "        data.at[date_B, '10Kdates'] = date_B\n",
    "        data.at[date_B, 'cosine_score'] = cosine_score\n",
    "        data.at[date_B, 'jaccard_score'] = jaccard_score\n",
    "\n",
    "        # Reset value for next loop\n",
    "        # (We don't open the file again, for efficiency)\n",
    "        file_text_A = file_text_B\n",
    "\n",
    "    # Save scores\n",
    "    os.chdir('../metrics')\n",
    "    data.to_csv(cik+'_sim_scores.csv', index=False)\n",
    "    os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeSimilarityScores10Q(cik):\n",
    "    \n",
    "    '''\n",
    "    Computes cosine and Jaccard similarity scores\n",
    "    over 10-Qs for a particular CIK.\n",
    "    \n",
    "    Compares each 10-Q to the 10-Q from the same\n",
    "    quarter of the previous year.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik: str\n",
    "        Central Index Key used to scrape and name\n",
    "        files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Define how stringent we want to be about \n",
    "    # \"previous year\"\n",
    "    year_short = timedelta(345)\n",
    "    year_long = timedelta(385)\n",
    "    \n",
    "    # Open directory that holds plain 10-Q textfiles\n",
    "    # for the CIK\n",
    "    os.chdir(cik+'/rawtext')\n",
    "    print(\"Parsing CIK %s...\" % cik)\n",
    "    \n",
    "    # Get list of files to compare\n",
    "    file_list = [fname for fname in os.listdir() if not \n",
    "                 (fname.startswith('.') | os.path.isdir(fname))]\n",
    "    file_list.sort()\n",
    "    \n",
    "    # Check if scores have already been calculated\n",
    "    try:\n",
    "        os.mkdir('../metrics')\n",
    "    # ... if they have already been calculated, exit\n",
    "    except OSError:\n",
    "        print(\"Already parsed CIK %s...\" % cik)\n",
    "        os.chdir('../..')\n",
    "        return\n",
    "    \n",
    "    # Check if enough files exist to compare\n",
    "    # ... if there aren't enough files, exit\n",
    "    if len(file_list) < 4:\n",
    "        print(\"No files to compare for CIK\", cik)\n",
    "        os.chdir('../..')\n",
    "        return\n",
    "    \n",
    "    # Initialize dataframe to hold similarity scores\n",
    "    dates = [x[-14:-4] for x in file_list]\n",
    "    cosine_score = [0]*len(dates)\n",
    "    jaccard_score = [0]*len(dates)\n",
    "    data = pd.DataFrame(columns={'cosine_score': cosine_score, \n",
    "                                 'jaccard_score': jaccard_score},\n",
    "                       index=dates)\n",
    "    \n",
    "    # Iterate over each quarter...\n",
    "    for j in range(3):\n",
    "        \n",
    "        # Get text and date of earliest filing from that quarter\n",
    "        file_name_A = file_list[j]\n",
    "        with open(file_name_A, 'r') as file:\n",
    "            file_text_A = file.read()\n",
    "        date_A = datetime.strptime(file_name_A[-14:-4], '%Y-%m-%d')\n",
    "        \n",
    "        # Iterate over the rest of the filings from that quarter...\n",
    "        for i in range(j+3, len(file_list), 3):\n",
    "\n",
    "            # Get name and date of the later file\n",
    "            file_name_B = file_list[i]\n",
    "            date_B = datetime.strptime(file_name_B[-14:-4], '%Y-%m-%d')\n",
    "            \n",
    "            # If B was not filed within ~1 year after A...\n",
    "            if (date_B > (date_A + year_long)) or (date_B < (date_A + year_short)):\n",
    "                \n",
    "                print(date_B.strftime('%Y-%m-%d'), \"is not within a year of\", date_A.strftime('%Y-%m-%d'))\n",
    "                \n",
    "                # Record values as NaN\n",
    "                data.at[date_B.strftime('%Y-%m-%d'), 'cosine_score'] = 'NaN'\n",
    "                data.at[date_B.strftime('%Y-%m-%d'), 'jaccard_score'] = 'NaN'\n",
    "                \n",
    "                # Pretend as if we found new date_A in the next year\n",
    "                date_A = date_A.replace(year=date_B.year)\n",
    "                \n",
    "                # Move to next filing\n",
    "                continue\n",
    "                \n",
    "            # If B was filed within ~1 year of A...\n",
    "            \n",
    "            # Get file text\n",
    "            with open(file_name_B, 'r') as file:\n",
    "                file_text_B = file.read()\n",
    "\n",
    "            # Get sets of words in A, B\n",
    "            words_A = set(re.findall(r\"[\\w']+\", file_text_A))\n",
    "            words_B = set(re.findall(r\"[\\w']+\", file_text_B))\n",
    "\n",
    "            # Calculate similarity score\n",
    "            cosine_score = ComputeCosineSimilarity(words_A, words_B)\n",
    "            jaccard_score = ComputeJaccardSimilarity(words_A, words_B)\n",
    "\n",
    "            # Store value (indexing by the date of document B)\n",
    "            data.at[date_B.strftime('%Y-%m-%d'), 'cosine_score'] = cosine_score\n",
    "            data.at[date_B.strftime('%Y-%m-%d'), 'jaccard_score'] = jaccard_score\n",
    "\n",
    "            # Reset value for next loop\n",
    "            # Don't re-read files, for efficiency\n",
    "            file_text_A = file_text_B\n",
    "            date_A = date_B\n",
    "\n",
    "    # Save scores\n",
    "    os.chdir('../metrics')\n",
    "    data.to_csv(cik+'_sim_scores.csv', index=True)\n",
    "    os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing CIK 0001045810...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Computing scores for 10-Ks...\n",
    "\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    ComputeSimilarityScores10K(cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing CIK 0001045810...\n",
      "2006-11-29 is not within a year of 2005-08-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# Computing scores for 10-Qs...\n",
    "\n",
    "os.chdir(pathname_10q)\n",
    "\n",
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    ComputeSimilarityScores10Q(cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetData(cik, pathname_10k, pathname_10q, pathname_data):\n",
    "    \n",
    "    '''\n",
    "    Consolidate 10-K and 10-Q data into a single dataframe\n",
    "    for a CIK.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik : str\n",
    "        Central Index Key used to scrape and\n",
    "        store data.\n",
    "    pathname_10k : str\n",
    "        Path to directory holding 10-K files.\n",
    "    pathname_10q : str\n",
    "        Path to directory holding 10-Q files.\n",
    "    pathname_data : str\n",
    "        Path to directory holding newly\n",
    "        generated data files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Flags to determine what data we have\n",
    "    data_10k = True\n",
    "    data_10q = True\n",
    "    \n",
    "    print(\"Gathering data for CIK %s...\" % cik)\n",
    "    file_name = ('%s_sim_scores_full.csv' % cik)\n",
    "    \n",
    "    # Check if data has already been gathered...\n",
    "    os.chdir(pathname_data)\n",
    "    file_list = [fname for fname in os.listdir() if not fname.startswith('.')]\n",
    "    \n",
    "    # ... if it has been, exit\n",
    "    if file_name in file_list:\n",
    "        print(\"Already gathered data for CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # Try to get 10-K data...\n",
    "    os.chdir(pathname_10k+'/%s/metrics' % cik)\n",
    "    try:\n",
    "        sim_scores_10k = pd.read_csv(cik+'_sim_scores.csv')\n",
    "    # ... if it doesn't exist, set 10-K flag to False\n",
    "    except FileNotFoundError:\n",
    "        print(\"No data to gather.\")\n",
    "        data_10k = False\n",
    "    \n",
    "    # Try to get 10-Q data...\n",
    "    os.chdir(pathname_10q+'/%s/metrics' % cik)\n",
    "    try:\n",
    "        sim_scores_10q = pd.read_csv(cik+'_sim_scores.csv')\n",
    "    # ... if it doesn't exist, set 10-Q flag to False\n",
    "    except FileNotFoundError:\n",
    "        print(\"No data to gather.\")\n",
    "        data_10q = False\n",
    "    \n",
    "    # Merge depending on available data...\n",
    "    # ... if there's no 10-K or 10-Q data, exit\n",
    "    if not (data_10k and data_10q):\n",
    "        return\n",
    "    \n",
    "    # ... if there's no 10-Q data (but there is 10-K data),\n",
    "    # only use the 10-K data\n",
    "    if not data_10q:\n",
    "        sim_scores = sim_scores_10k\n",
    "    # ... if the opposite is true, only use 10-Q data\n",
    "    elif not data_10k:\n",
    "        sim_scores = sim_scores_10q\n",
    "    # ... if there's both 10-K and 10-Q data, merge\n",
    "    elif (data_10q and data_10k):\n",
    "        sim_scores = pd.concat([sim_scores_10k, sim_scores_10q], \n",
    "                           axis='index')\n",
    "    \n",
    "    # Rename date column\n",
    "    sim_scores.rename(columns={'Unnamed: 0': '10Qdates'}, inplace=True)\n",
    "\n",
    "    # Set CIK column\n",
    "    sim_scores['cik'] = cik\n",
    "    \n",
    "    # Save file in the data dir\n",
    "    os.chdir(pathname_data)\n",
    "    sim_scores.to_csv('%s_sim_scores_full.csv' % cik, index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname_data = '/Users/Kyelee/MyProjects/10K_NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 93.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data for CIK 0001045810...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for cik in tqdm(ticker_cik_df['cik']):\n",
    "    GetData(cik, pathname_10k, pathname_10q, pathname_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Kyelee/MyProjects/10K_NLP\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd() \n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
